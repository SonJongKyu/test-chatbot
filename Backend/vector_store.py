# --------------------------------------------------
# File: ~/RAG_Chatbot/Backend/vector_store.py
# Description: FAISS ê¸°ë°˜ ë²¡í„° DB + ì „ëµ ê¸°ë°˜ ì²­í‚¹ ëŒ€ì‘ ì™„ì „ ì§€ì›
# --------------------------------------------------

import faiss
import json
import os
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer

# ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.join(os.path.expanduser("~"), "RAG_Chatbot")
DB_DIR = os.path.join(BASE_DIR, "faiss_db")
os.makedirs(DB_DIR, exist_ok=True)

FAISS_PATH = os.path.join(DB_DIR, "vector.index")
METADATA_PATH = os.path.join(DB_DIR, "metadata.json")
MODEL_NAME = "BAAI/bge-m3"

# ì „ì—­ ë³€ìˆ˜
faiss_index = None
metadata = []
embedder = None


# ===== Embedding ëª¨ë¸ & FAISS ë¡œë“œ =====
def load_faiss_into_memory():
    global faiss_index, metadata, embedder

    print("ğŸ”µ Loading embedding model on CPU...")
    embedder = SentenceTransformer(MODEL_NAME, device="cpu")
    print("ğŸŸ¢ Embedding model loaded.")

    # Load FAISS
    if os.path.exists(FAISS_PATH):
        try:
            faiss_index = faiss.read_index(FAISS_PATH)
            print(f"ğŸŸ¢ FAISS index loaded. Total vectors: {faiss_index.ntotal}")
        except Exception as e:
            print(f"âŒ Failed to load FAISS index: {e}")
            faiss_index = None
    else:
        print("âšª No FAISS index found. Starting fresh.")
        faiss_index = None

    # Load metadata
    if os.path.exists(METADATA_PATH):
        try:
            with open(METADATA_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                metadata[:] = data if isinstance(data, list) else []
            print(f"ğŸŸ¢ Metadata loaded. Total chunks = {len(metadata)}")
        except Exception as e:
            print(f"âŒ Metadata load error: {e}")
            metadata[:] = []
    else:
        metadata[:] = []
        print("âšª No metadata found. Starting fresh.")


# ===== Metadata ì €ì¥ =====
def save_metadata():
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)


# ===== chunk â†’ ì„ë² ë”© ë¬¸ìì—´ ë³€í™˜ =====
def extract_text_for_embedding(chunk: dict) -> str:
    """
    CSV ë ˆì½”ë“œì²˜ëŸ¼ text í•„ë“œê°€ ì—†ëŠ” chunkë„ ì§€ì›í•˜ë„ë¡
    ê°€ì¥ ê¸´ ë¬¸ìì—´ì„ ëŒ€í‘œ í…ìŠ¤íŠ¸ë¡œ ì„ íƒ.
    """
    if "text" in chunk and isinstance(chunk["text"], str) and chunk["text"].strip():
        return chunk["text"]

    # ë¬¸ìì—´ í•„ë“œë“¤ ì¤‘ ê°€ì¥ ê¸´ ê°’ ì„ íƒ
    values = [v for v in chunk.values() if isinstance(v, str)]
    if values:
        return max(values, key=len)

    # ë¬¸ìì—´ í•„ë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    return json.dumps(chunk, ensure_ascii=False)


# ===== ì„ë² ë”© ìƒì„± =====
def embed_texts(text_list):
    embeddings = embedder.encode(text_list, convert_to_numpy=True, batch_size=16)
    return embeddings.astype("float32")


# ===== ë²¡í„° / ë©”íƒ€ë°ì´í„° ì €ì¥ =====
def save_faiss(chunks, file_name: str):
    """
    chunks = apply_chunk_strategy() ê²°ê³¼ ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜´
    JSON êµ¬ì¡°ê°€ ëª¨ë‘ ë‹¤ë¥´ë”ë¼ë„ ì €ì¥ ê°€ëŠ¥í•´ì•¼ í•¨
    """
    global faiss_index, metadata

    if chunks is None or len(chunks) == 0:
        print(f"âš  ì €ì¥í•  ì²­í¬ ì—†ìŒ: {file_name}")
        return

    existing_hashes = {m.get("hash", "") for m in metadata}

    embedding_texts = []
    new_meta = []

    for c in chunks:
        embed_text = extract_text_for_embedding(c)
        h = hashlib.md5(embed_text.encode("utf-8")).hexdigest()

        if h in existing_hashes:
            continue

        embedding_texts.append(embed_text)
        new_meta.append({
            "id": len(metadata) + len(new_meta),
            "file_name": file_name,
            **c,               # ğŸ”¥ ì²­í¬ JSON ì „ì²´ ê·¸ëŒ€ë¡œ ë³´ì¡´
            "hash": h
        })

    if not embedding_texts:
        print("âšª ëª¨ë“  ì²­í¬ê°€ ì¤‘ë³µ â€” ì €ì¥ ìƒëµ")
        return

    vectors = embed_texts(embedding_texts)
    dim = vectors.shape[1]

    if faiss_index is None or faiss_index.ntotal == 0:
        index = faiss.IndexFlatL2(dim)
        index.add(vectors)
        faiss_index = index
    else:
        existing_vectors = faiss_index.reconstruct_n(0, faiss_index.ntotal)
        index = faiss.IndexFlatL2(dim)
        index.add(existing_vectors)
        index.add(vectors)
        faiss_index = index

    metadata.extend(new_meta)

    faiss.write_index(faiss_index, FAISS_PATH)
    save_metadata()

    print(f"ğŸŸ¢ ì €ì¥ ì™„ë£Œ â€” íŒŒì¼: {file_name}, ìƒˆ ì²­í¬: {len(new_meta)}, ì „ì²´: {faiss_index.ntotal}")


# ===== ì „ì²´ ì¬ì¸ë±ì‹± =====
def rebuild_faiss_from_metadata(new_metadata):
    global metadata, faiss_index

    cleaned = []
    for i, m in enumerate(new_metadata):
        row = dict(m)
        row["id"] = i
        cleaned.append(row)

    metadata = cleaned

    if not metadata:
        dim = embedder.get_sentence_embedding_dimension()
        faiss_index = faiss.IndexFlatL2(dim)
        faiss.write_index(faiss_index, FAISS_PATH)
        save_metadata()
        print("ğŸŸ¢ ë¹ˆ ì¸ë±ìŠ¤ë¡œ ì´ˆê¸°í™”ë¨")
        return

    text_list = [extract_text_for_embedding(m) for m in metadata]
    vectors = embedder.encode(text_list, convert_to_numpy=True).astype("float32")

    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)
    faiss_index = index

    faiss.write_index(faiss_index, FAISS_PATH)
    save_metadata()

    print(f"ğŸŸ¢ ì „ì²´ ì¬ì¸ë±ì‹± ì™„ë£Œ â€” ì´ ë²¡í„°: {faiss_index.ntotal}")


# ===== ê²€ìƒ‰ =====
def search_faiss(query, top_k=3):
    global metadata, faiss_index

    if faiss_index is None:
        raise RuntimeError("FAISS index not initialized!")

    q_vec = embedder.encode([query], convert_to_numpy=True).astype("float32")
    D, I = faiss_index.search(q_vec, top_k)

    results = []
    for idx, score in zip(I[0], D[0]):
        if 0 <= idx < len(metadata):
            chunk = metadata[idx]
            results.append({**chunk, "score": float(score)})

    return results
